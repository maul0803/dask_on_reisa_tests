slurmstepd: info: Setting TMPDIR to /scratch/10942657. Previous errors about TMPDIR can be discarded
Slurm job started at 13/03/2025_01:25:35 AM

In situ 2,
Initing Ray (1 head node + 8 worker nodes + 16 simulation nodes) on nodes: 
	ilk-61 ilk-65 ilk-71 ilk-74 ilk-79 ilk-93 ilk-94 ilk-145 ilk-146 ilk-147 ilk-148 ilk-150 ilk-151 ilk-152 ilk-153 ilk-154 ilk-156 ilk-157 ilk-158 ilk-159 ilk-161 ilk-167 ilk-169 ilk-175 ilk-186
Head node: 172.30.202.13:6379


RAY_DEPLOY_TIME:     23.47831
2025-03-13 01:26:03,418	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E6/DASK_ON_REISA/derivative/P512-SN16-LS134-GS68719-I10-AN8-D2025-03-12_10-03-47'.
2025-03-13 01:26:03,446	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_0818f2cec8970d54.zip' (0.48MiB) to Ray cluster...
2025-03-13 01:26:03,452	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_0818f2cec8970d54.zip'.
RAY_INIT_TIME:       7.38440
Iter [0]
Iter [1]
[2m[36m(iter_task pid=3904305, ip=10.120.202.26)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=3904305, ip=10.120.202.26)[0m   warnings.warn(
[2m[36m(iter_task pid=3781638, ip=10.120.202.45)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=3781638, ip=10.120.202.45)[0m   warnings.warn(
[2m[36m(iter_task pid=3781639, ip=10.120.202.45)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=3781639, ip=10.120.202.45)[0m   warnings.warn(
[2m[36m(iter_task pid=2552977, ip=10.120.202.31)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=2552977, ip=10.120.202.31)[0m   warnings.warn(
[2m[36m(iter_task pid=2552978, ip=10.120.202.31)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=2552978, ip=10.120.202.31)[0m   warnings.warn(
[2m[36m(iter_task pid=2552979, ip=10.120.202.31)[0m /home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/dask/config.py:693: UserWarning: Configuration key "shuffle" has been deprecated. Please use "dataframe.shuffle.algorithm" instead
[2m[36m(iter_task pid=2552979, ip=10.120.202.31)[0m   warnings.warn(
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
GLOBAL_PUT_TIME:     7.015939474105835 (avg:0.70159)
ACTOR_CONCURRENCY:   8
SIMULATION_TIME:     280.118841007000015 (avg: 28.011884100700001)
SIM_WTHOUT_PDI:      30.099080109999992 (avg: 3.009908010999999)
PDI_DELAY:           250.019760897000026 (avg: 25.001976089700001)

GLOBAL_SIZE_(GiB):   67108864
LOCAL_SIZE_(MiB):    128
ITERATIONS:          10

MPI_PER_NODE:        32
MPI_PARALLELISM:     512

WORKER_NODES:        8
CPUS_PER_WORKER:     64
WORKER_PARALLELISM:  512


SLURM_JOB_ID:        10942657
WARNING: 12 PYTHON worker processes have been started on node: 94ccb7b88db4e0f9ea8848003cdb6e32da904e64777fb3cc897bafbf with address: 172.30.204.8. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: b3c64370c91243d1680a69a67de86291f81a6d463a3cd8b16df6ef16 with address: 172.30.204.3. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: 518375f87dc0fd1086e37ff4fc601a76ef599d88e9307c09135cb24a with address: 172.30.204.14. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: 19fd8309369363e38362a7965037deb58a423a15c2a4a751cb360a87 with address: 172.30.204.6. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: 99140bec6091f77fd76344d4bc31a5ee45a5024cbc58b302aad13024 with address: 172.30.204.13. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: ca27898d666f81a3093ebb22dd0a44f07712e5e9e54f89443ce85da0 with address: 172.30.204.23. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: 69e3b5def32c9bcc0ad0d08d06068e1e15ab91a1b176057d6d2aaeba with address: 172.30.204.4. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: a2b132e8823e7f1101778ad831083b11f185a643c9c9ec307f55e029 with address: 172.30.204.17. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: ac225119ea5f170093d411db88b511a66d494239e5fdfe9c6c2bbbc0 with address: 172.30.204.31. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
WARNING: 12 PYTHON worker processes have been started on node: 59742b68585a86c4e6d915cd7a94b12a9f0a91f1c10153d785e53226 with address: 172.30.204.12. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 10942657 ON ilk-61 CANCELLED AT 2025-03-13T01:45:57 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 10942657.26 ON ilk-65 CANCELLED AT 2025-03-13T01:45:57 DUE TO TIME LIMIT ***
