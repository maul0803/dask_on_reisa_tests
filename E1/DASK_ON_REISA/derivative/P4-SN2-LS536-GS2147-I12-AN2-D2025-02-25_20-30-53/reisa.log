slurmstepd: info: Setting TMPDIR to /scratch/10772447. Previous errors about TMPDIR can be discarded
Slurm job started at 26/02/2025_12:32:38 AM

In transit.
Initing Ray (1 head node + 2 worker nodes + 2 simulation nodes) on nodes: 
	ilk-104 ilk-105 ilk-106 ilk-107 ilk-108
Head node: 172.30.203.8:6379


RAY_DEPLOY_TIME:     22.43896
RAY_INIT_TIME:       3.78046
Iter [0]
2025-02-26 00:33:06,469	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/derivative/P4-SN2-LS536-GS2147-I12-AN2-D2025-02-25_20-30-53'.
2025-02-26 00:33:06,496	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_d4f8e52c3d1716b4.zip' (0.48MiB) to Ray cluster...
2025-02-26 00:33:06,503	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_d4f8e52c3d1716b4.zip'.
Iter [1]
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
Iter [10]
Iter [11]
GLOBAL_PUT_TIME:     9.199501514434814 (avg:0.76663)
ACTOR_CONCURRENCY:   8
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3489123, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=2981669, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3488838, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3498703, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3498703, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2980855, ip=10.120.203.10)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503622, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503622, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503622, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
SIMULATION_TIME:     85.312018373000001 (avg: 7.109334864416667)
SIM_WTHOUT_PDI:      66.525330477000011 (avg: 5.543777539750001)
PDI_DELAY:           18.786687895999989 (avg: 1.565557324666666)

GLOBAL_SIZE_(GiB):   2097152
LOCAL_SIZE_(MiB):    512
ITERATIONS:          12

MPI_PER_NODE:        2
MPI_PARALLELISM:     4

WORKER_NODES:        2
CPUS_PER_WORKER:     30
WORKER_PARALLELISM:  60


SLURM_JOB_ID:        10772447
EST_ANALYTICS_TIME:  185.31310 (avg:15.44276)

Slurm job finished at 26/02/2025_12:36:14 AM

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772447)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772447
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 5
Cores per node: 40
CPU Utilized: 00:09:15
CPU Efficiency: 1.26% of 12:16:40 core-walltime
Job Wall-clock time: 00:03:41
Memory Utilized: 6.22 GB
Memory Efficiency: 0.78% of 800.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772447)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772447
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 5
Cores per node: 40
CPU Utilized: 00:52:16
CPU Efficiency: 7.10% of 12:16:40 core-walltime
Job Wall-clock time: 00:03:41
Memory Utilized: 44.74 GB
Memory Efficiency: 5.59% of 800.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772447)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772447
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 5
Cores per node: 40
CPU Utilized: 00:52:16
CPU Efficiency: 7.10% of 12:16:40 core-walltime
Job Wall-clock time: 00:03:41
Memory Utilized: 44.74 GB
Memory Efficiency: 5.59% of 800.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772447)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772447
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 5
Cores per node: 40
CPU Utilized: 00:52:16
CPU Efficiency: 7.10% of 12:16:40 core-walltime
Job Wall-clock time: 00:03:41
Memory Utilized: 44.74 GB
Memory Efficiency: 5.59% of 800.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772447)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772447
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 5
Cores per node: 40
CPU Utilized: 00:52:16
CPU Efficiency: 7.10% of 12:16:40 core-walltime
Job Wall-clock time: 00:03:41
Memory Utilized: 44.74 GB
Memory Efficiency: 5.59% of 800.00 GB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************

