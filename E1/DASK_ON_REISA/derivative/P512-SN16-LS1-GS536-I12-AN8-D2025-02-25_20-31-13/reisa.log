slurmstepd: info: Setting TMPDIR to /scratch/10772505. Previous errors about TMPDIR can be discarded
Slurm job started at 25/02/2025_08:51:25 PM

In transit.
Initing Ray (1 head node + 8 worker nodes + 16 simulation nodes) on nodes: 
	ilk-97 ilk-98 ilk-99 ilk-100 ilk-101 ilk-102 ilk-103 ilk-104 ilk-105 ilk-106 ilk-126 ilk-138 ilk-155 ilk-161 ilk-162 ilk-166 ilk-167 ilk-168 ilk-170 ilk-178 ilk-243 ilk-244 ilk-245 ilk-246 ilk-247
Head node: 172.30.203.1:6379


RAY_DEPLOY_TIME:     23.70512
2025-02-25 20:51:54,802	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/derivative/P512-SN16-LS1-GS536-I12-AN8-D2025-02-25_20-31-13'.
2025-02-25 20:51:54,852	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_8dbe635646607bad.zip' (0.48MiB) to Ray cluster...
2025-02-25 20:51:54,860	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_8dbe635646607bad.zip'.
RAY_INIT_TIME:       9.16279
Iter [0]
Iter [1]
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
Iter [10]
Iter [11]
GLOBAL_PUT_TIME:     0.05891108512878418 (avg:0.00491)
ACTOR_CONCURRENCY:   8
[2m[36m(process_task pid=987589, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987589, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987589, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2834614, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 57x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2834614, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 57x across cluster][0m
[2m[36m(process_task pid=2834614, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 57x across cluster][0m
[2m[36m(process_task pid=3124585, ip=10.120.203.8)[0m result:
[2m[36m(process_task pid=987591, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987591, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987591, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3826298, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 49x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3826298, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>[32m [repeated 49x across cluster][0m
[2m[36m(process_task pid=3826298, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 49x across cluster][0m
[2m[36m(process_task pid=3126287, ip=10.120.203.8)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 62x across cluster][0m
[2m[36m(process_task pid=3126287, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 61x across cluster][0m
[2m[36m(process_task pid=3126287, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 61x across cluster][0m
[2m[36m(process_task pid=2578765, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578765, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578765, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2932906, ip=10.120.203.6)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 48x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2932438, ip=10.120.203.6)[0m result: <class 'dask.array.core.Array'>[32m [repeated 47x across cluster][0m
[2m[36m(process_task pid=2932438, ip=10.120.203.6)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 47x across cluster][0m
[2m[36m(process_task pid=2833473, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833473, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833473, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=1012248, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=1012248, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=1012248, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=2581161, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster][0m
[2m[36m(process_task pid=2833473, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster][0m
[2m[36m(process_task pid=2833473, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster][0m
[2m[36m(process_task pid=990296, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 33x across cluster][0m
[2m[36m(process_task pid=990296, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>[32m [repeated 34x across cluster][0m
[2m[36m(process_task pid=990296, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 34x across cluster][0m
[2m[36m(process_task pid=3276960, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276960, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276960, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2834554, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2834554, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=2834554, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=2834554, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=2585251, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=2585251, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=2833474, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833474, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833474, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2580893, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 70x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2580893, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>[32m [repeated 70x across cluster][0m
[2m[36m(process_task pid=2580893, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 70x across cluster][0m
[2m[36m(process_task pid=1012249, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 42x across cluster][0m
[2m[36m(process_task pid=1012249, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 42x across cluster][0m
[2m[36m(process_task pid=1012249, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 42x across cluster][0m
[2m[36m(process_task pid=3124585, ip=10.120.203.8)[0m  <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3282046, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 61x across cluster][0m
[2m[36m(process_task pid=3282046, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>[32m [repeated 60x across cluster][0m
[2m[36m(process_task pid=3282046, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 61x across cluster][0m
[2m[36m(process_task pid=2833471, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833471, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833471, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2836476, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 67x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2836476, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 67x across cluster][0m
[2m[36m(process_task pid=2836476, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 67x across cluster][0m
[2m[36m(process_task pid=2833478, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833478, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833478, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2838410, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 68x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2838410, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 68x across cluster][0m
[2m[36m(process_task pid=2838410, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 68x across cluster][0m
[2m[36m(process_task pid=2833475, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833475, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833475, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2580564, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 57x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2838216, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster][0m
[2m[36m(process_task pid=2838216, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster][0m
[2m[36m(process_task pid=3121883, ip=10.120.203.8)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 60x across cluster][0m
[2m[36m(process_task pid=3121883, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=3121883, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>[32m [repeated 63x across cluster][0m
[2m[36m(process_task pid=2578764, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 63x across cluster][0m
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2578762, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3281732, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 81x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3281732, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>[32m [repeated 81x across cluster][0m
[2m[36m(process_task pid=3281732, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 81x across cluster][0m
[2m[36m(process_task pid=989937, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 23x across cluster][0m
[2m[36m(process_task pid=989937, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>[32m [repeated 23x across cluster][0m
[2m[36m(process_task pid=989937, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 23x across cluster][0m
[2m[36m(process_task pid=3122255, ip=10.120.203.8)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 46x across cluster][0m
[2m[36m(process_task pid=3122255, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 46x across cluster][0m
[2m[36m(process_task pid=3122255, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 46x across cluster][0m
[2m[36m(process_task pid=2585848, ip=10.120.203.2)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 60x across cluster][0m
[2m[36m(process_task pid=2585848, ip=10.120.203.2)[0m result: <class 'dask.array.core.Array'>[32m [repeated 60x across cluster][0m
[2m[36m(process_task pid=2585848, ip=10.120.203.2)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 60x across cluster][0m
[2m[36m(process_task pid=3122392, ip=10.120.203.8)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=3126194, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=3126194, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=2833472, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833472, ip=10.120.203.5)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2833472, ip=10.120.203.5)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2838104, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 56x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=1012185, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 55x across cluster][0m
[2m[36m(process_task pid=1012185, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 55x across cluster][0m
[2m[36m(process_task pid=2838104, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=3126154, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=3126154, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=3276961, ip=10.120.203.9)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276961, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276961, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276961, ip=10.120.203.9)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3276961, ip=10.120.203.9)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3122252, ip=10.120.203.8)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3122252, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 63x across cluster][0m
[2m[36m(process_task pid=3122252, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 63x across cluster][0m
[2m[36m(process_task pid=1016378, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 53x across cluster][0m
[2m[36m(process_task pid=1016378, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=1016378, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 52x across cluster][0m
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824913, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=1011353, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=1011353, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=1011353, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 51x across cluster][0m
[2m[36m(process_task pid=3831506, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 67x across cluster][0m
[2m[36m(process_task pid=3831647, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=3831647, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 66x across cluster][0m
[2m[36m(process_task pid=987592, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987592, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=987592, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2836308, ip=10.120.203.5)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 62x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=1014366, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster][0m
[2m[36m(process_task pid=1014366, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 64x across cluster][0m
[2m[36m(process_task pid=1014366, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 61x across cluster][0m
[2m[36m(process_task pid=3126286, ip=10.120.203.8)[0m result: <class 'dask.array.core.Array'>[32m [repeated 59x across cluster][0m
[2m[36m(process_task pid=3126286, ip=10.120.203.8)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 59x across cluster][0m
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3824912, ip=10.120.203.3)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=990334, ip=10.120.203.7)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=990334, ip=10.120.203.7)[0m result: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster][0m
[2m[36m(process_task pid=990334, ip=10.120.203.7)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 43x across cluster][0m
[2m[36m(process_task pid=1017732, ip=10.120.203.4)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 79x across cluster][0m
[2m[36m(process_task pid=1017732, ip=10.120.203.4)[0m result: <class 'dask.array.core.Array'>[32m [repeated 79x across cluster][0m
[2m[36m(process_task pid=1017732, ip=10.120.203.4)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 79x across cluster][0m
SIMULATION_TIME:     16.335280740999998 (avg: 1.361273395083333)
SIM_WTHOUT_PDI:      5.889941106999999 (avg: 0.490828425583333)
PDI_DELAY:           10.445339634000000 (avg: 0.870444969500000)

GLOBAL_SIZE_(GiB):   524288
LOCAL_SIZE_(MiB):    1
ITERATIONS:          12

MPI_PER_NODE:        32
MPI_PARALLELISM:     512

WORKER_NODES:        8
CPUS_PER_WORKER:     30
WORKER_PARALLELISM:  240


SLURM_JOB_ID:        10772505
EST_ANALYTICS_TIME:  42.06181 (avg:3.50515)

Slurm job finished at 25/02/2025_08:52:43 PM

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:12:17
CPU Efficiency: 9.92% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 586.02 MB
Memory Efficiency: 0.01% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:25:23
CPU Efficiency: 10.90% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 02:27:44
CPU Efficiency: 11.08% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 1.88 GB
Memory Efficiency: 0.05% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772505)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772505
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 25
Cores per node: 40
CPU Utilized: 04:04:02
CPU Efficiency: 18.30% of 22:13:20 core-walltime
Job Wall-clock time: 00:01:20
Memory Utilized: 5.85 GB
Memory Efficiency: 0.15% of 3.91 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************

