slurmstepd: info: Setting TMPDIR to /scratch/10772501. Previous errors about TMPDIR can be discarded
Slurm job started at 25/02/2025_11:02:29 PM

In transit.
Initing Ray (1 head node + 8 worker nodes + 8 simulation nodes) on nodes: 
	ilk-4 ilk-89 ilk-91 ilk-151 ilk-152 ilk-159 ilk-161 ilk-162 ilk-166 ilk-167 ilk-170 ilk-172 ilk-173 ilk-177 ilk-178 ilk-179 ilk-185
Head node: 172.30.201.4:6379


RAY_DEPLOY_TIME:     23.61602
RAY_INIT_TIME:       3.43606
Iter [0]
2025-02-25 23:03:01,928	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/derivative/P16-SN8-LS536-GS8589-I12-AN8-D2025-02-25_20-31-05'.
2025-02-25 23:03:01,956	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_6ec2c266a7a4f7f1.zip' (0.48MiB) to Ray cluster...
2025-02-25 23:03:01,962	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_6ec2c266a7a4f7f1.zip'.
Iter [1]
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
Iter [10]
Iter [11]
GLOBAL_PUT_TIME:     9.302057981491089 (avg:0.77517)
ACTOR_CONCURRENCY:   8
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124387, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2128051, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2128051, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2128051, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503835, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503835, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503835, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267223, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267223, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267223, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>[32m [repeated 3x across cluster][0m
[2m[36m(process_task pid=675203, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 3x across cluster][0m
[2m[36m(process_task pid=267330, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267330, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267330, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672790, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2127131, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127131, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503264, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817213, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264765, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822070, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822070, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822070, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822425, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822425, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3822425, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673645, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263395, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814201, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3814320, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3509052, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3509052, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3509052, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2124672, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=674734, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264195, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3813499, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3821346, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3821346, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3821346, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672791, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2126337, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2126337, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2126337, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502650, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812796, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812796, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812796, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816103, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264885, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=271433, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=271433, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=271433, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672500, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2127275, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=264194, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3503834, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267591, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267591, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=267591, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817707, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817707, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3817707, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3819600, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3819600, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3819600, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673933, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673933, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=673933, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=2125334, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3502558, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3816698, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816698, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3816698, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263334, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2136312, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2136312, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2136312, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
SIMULATION_TIME:     460.483988660000023 (avg: 38.373665721666669)
SIM_WTHOUT_PDI:      65.134073554000011 (avg: 5.427839462833334)
PDI_DELAY:           395.349915106000026 (avg: 32.945826258833335)

GLOBAL_SIZE_(GiB):   8388608
LOCAL_SIZE_(MiB):    512
ITERATIONS:          12

MPI_PER_NODE:        2
MPI_PARALLELISM:     16

WORKER_NODES:        8
CPUS_PER_WORKER:     30
WORKER_PARALLELISM:  240


SLURM_JOB_ID:        10772501
[2m[36m(process_task pid=672789, ip=10.120.204.15)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672789, ip=10.120.204.15)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=672789, ip=10.120.204.15)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125275, ip=10.120.202.43)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125275, ip=10.120.202.43)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=2125275, ip=10.120.202.43)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3506170, ip=10.120.202.41)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[2m[36m(process_task pid=3506170, ip=10.120.202.41)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3506170, ip=10.120.202.41)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>[32m [repeated 2x across cluster][0m
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=263905, ip=10.120.204.7)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3815803, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m gt: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result: <class 'dask.array.core.Array'>
[2m[36m(process_task pid=3812795, ip=10.120.204.18)[0m result_dask: <class 'dask.array.core.Array'>
EST_ANALYTICS_TIME:  705.79816 (avg:58.81651)

Slurm job finished at 25/02/2025_11:14:51 PM

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 02:01:26
CPU Efficiency: 1.44% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 20.67 GB (estimated maximum)
Memory Efficiency: 0.76% of 2.66 TB (4.00 GB/core)

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 02:02:22
CPU Efficiency: 1.45% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 20.67 GB (estimated maximum)
Memory Efficiency: 0.76% of 2.66 TB (4.00 GB/core)

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 02:02:22
CPU Efficiency: 1.45% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 20.67 GB (estimated maximum)
Memory Efficiency: 0.76% of 2.66 TB (4.00 GB/core)

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 02:10:24
CPU Efficiency: 1.55% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 4.59 GB
Memory Efficiency: 0.17% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************



*****************************************************************************
*****************************************************************************
*                                                                           *
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*                                                                           *
*****************************************************************************
*****************************************************************************


Job ID: 10772501
Job ID: 10772501
Cluster: finisterrae3
Cluster: finisterrae3
User/Group: curso341/ulc
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
Nodes: 17
Nodes: 17
Cores per node: 40
Cores per node: 40
CPU Utilized: 04:12:42
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB
Memory Efficiency: 1.78% of 2.66 TB


 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
 
*****************************************************************************
*****************************************************************************



*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 04:12:42
CPU Efficiency: 3.00% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 05:21:36
CPU Efficiency: 3.82% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 05:21:36
CPU Efficiency: 3.82% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************


*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10772501)                   *
*                                                                           *
*****************************************************************************

Job ID: 10772501
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 17
Cores per node: 40
CPU Utilized: 05:21:36
CPU Efficiency: 3.82% of 5-20:20:40 core-walltime
Job Wall-clock time: 00:12:23
Memory Utilized: 48.31 GB
Memory Efficiency: 1.78% of 2.66 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************

