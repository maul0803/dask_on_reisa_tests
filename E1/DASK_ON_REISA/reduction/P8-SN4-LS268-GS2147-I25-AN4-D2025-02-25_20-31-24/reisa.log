slurmstepd: info: Setting TMPDIR to /scratch/10772509. Previous errors about TMPDIR can be discarded
Slurm job started at 25/02/2025_11:08:59 PM

In transit.
Initing Ray (1 head node + 4 worker nodes + 4 simulation nodes) on nodes: 
	ilk-65 ilk-67 ilk-68 ilk-82 ilk-88 ilk-122 ilk-126 ilk-131 ilk-138
Head node: 172.30.202.17:6379


RAY_DEPLOY_TIME:     22.41453
2025-02-25 23:09:27,416	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/reduction/P8-SN4-LS268-GS2147-I25-AN4-D2025-02-25_20-31-24'.
2025-02-25 23:09:27,444	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_332794bb79fc2e0c.zip' (0.48MiB) to Ray cluster...
2025-02-25 23:09:27,451	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_332794bb79fc2e0c.zip'.
RAY_INIT_TIME:       4.77307
Iter [0]
Iter [2]
Iter [4]
Iter [6]
Iter [8]
Iter [10]
Iter [12]
Iter [14]
Iter [16]
Iter [18]
Iter [20]
Iter [22]
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m 2025-02-25 23:13:30,743	WARNING worker.py:2519 -- Local object store memory usage:
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m 
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m (global lru) capacity: 72000000000
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m (global lru) used: 0%
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m (global lru) num objects: 0
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m (global lru) num evictions: 358
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m (global lru) bytes evicted: 165195496115
[2m[36m(iter_task pid=635495, ip=10.120.202.19)[0m 
The node with node id: 4de2d12f4d3b2493cda5f2720f216baf8b9e15a562e384a598145be4 and address: 172.30.203.26 and node name: 172.30.203.26 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a 	(1) raylet crashes unexpectedly (OOM, preempted node, etc.) 
	(2) raylet has lagging heartbeats due to slow network or busy workload.
EST_ANALYTICS_TIME:  256.26608 (avg:10.25064)
Traceback (most recent call last):
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/reduction/P8-SN4-LS268-GS2147-I25-AN4-D2025-02-25_20-31-24/reduction.py", line 56, in <module>
    result = handler.get_result(process_func, iter_func, selected_iters=iterations, kept_iters=max_iterations,
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/reduction/P8-SN4-LS268-GS2147-I25-AN4-D2025-02-25_20-31-24/reisa.py", line 127, in get_result
    tmp = ray.get(results)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return getattr(ray, func.__name__)(*args, **kwargs)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/api.py", line 42, in get
    return self.worker.get(vals, timeout=timeout)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/worker.py", line 434, in get
    res = self._get(to_get, op_timeout)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/worker.py", line 462, in _get
    raise err
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/server/server.py", line 489, in _get_object
    items = ray.get(objectrefs, timeout=request.timeout)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/_private/worker.py", line 2521, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: [36mray::iter_task()[39m (pid=635495, ip=172.30.202.19)
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA/reduction/P8-SN4-LS268-GS2147-I25-AN4-D2025-02-25_20-31-24/reisa.py", line 116, in iter_task
    current_results_list = ray.get(current_results_list)  # type: #List[dask.array.core.Array]
ray.exceptions.OwnerDiedError: Failed to retrieve object 192812bb0ba738c7ffffffffffffffffffffffff0300000001000000. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

The object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*2bf97ec6f7c163285967e885e030970d315c2a80d681f3b0893fa4db*` at IP address 172.30.203.30) for more information about the Python worker failure.
srun: error: ilk-67: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=10772509.10
slurmstepd: error: *** JOB 10772509 ON ilk-65 CANCELLED AT 2025-02-25T23:29:12 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 10772509.11 ON ilk-122 CANCELLED AT 2025-02-25T23:29:12 DUE TO TIME LIMIT ***
*** SIGTERM received at time=1740522552 on cpu 20 ***
*** SIGTERM received at time=1740522552 on cpu 12 ***
*** SIGTERM received at time=1740522552 on cpu 0 ***
*** SIGTERM received at time=1740522552 on cpu 14 ***
*** SIGTERM received at time=1740522552 on cpu 16 ***
[failure_signal_handler.cc : 329] RAW: Signal 15 raised at PC=0x14c254ef254c while already in AbslFailureSignalHandler()
*** SIGTERM received at time=1740522552 on cpu 20 ***
[failure_signal_handler.cc : 329] RAW: Signal 15 raised at PC=0x149d8c6f254c while already in AbslFailureSignalHandler()
*** SIGTERM received at time=1740522552 on cpu 12 ***
