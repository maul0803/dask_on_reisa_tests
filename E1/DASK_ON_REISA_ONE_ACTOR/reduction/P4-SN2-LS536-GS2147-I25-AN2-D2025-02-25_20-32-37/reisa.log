slurmstepd: info: Setting TMPDIR to /scratch/10772538. Previous errors about TMPDIR can be discarded
Slurm job started at 26/02/2025_12:39:27 AM

In transit.
Initing Ray (1 head node + 2 worker nodes + 2 simulation nodes) on nodes: 
	ilk-109 ilk-114 ilk-115 ilk-117 ilk-120
Head node: 172.30.203.13:6379


RAY_DEPLOY_TIME:     22.61341
2025-02-26 00:39:54,441	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/DASK_ON_REISA_ONE_ACTOR/reduction/P4-SN2-LS536-GS2147-I25-AN2-D2025-02-25_20-32-37'.
2025-02-26 00:39:54,468	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_abdea639e26e4c84.zip' (0.48MiB) to Ray cluster...
2025-02-26 00:39:54,475	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_abdea639e26e4c84.zip'.
RAY_INIT_TIME:       3.95985
Iter [0]
Iter [2]
Iter [4]
Iter [6]
Iter [8]
Iter [10]
Iter [12]
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m 2025-02-26 00:42:37,888	WARNING worker.py:2519 -- Local object store memory usage:
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m 
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m (global lru) capacity: 72000000000
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m (global lru) used: 40.3146%
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m (global lru) num objects: 24
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m (global lru) num evictions: 16
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m (global lru) bytes evicted: 56908377768
[2m[36m(iter_task pid=2016694, ip=10.120.203.18)[0m 
Iter [14]
Iter [16]
Iter [18]
Iter [20]
Iter [22]
Iter [24]
GLOBAL_PUT_TIME:     15.835777282714844 (avg:0.63343)
ACTOR_CONCURRENCY:   8
SIMULATION_TIME:     272.806569903999957 (avg: 10.912262796159999)
SIM_WTHOUT_PDI:      120.824852074999967 (avg: 4.832994082999999)
PDI_DELAY:           151.981717828999990 (avg: 6.079268713159999)

GLOBAL_SIZE_(GiB):   2097152
LOCAL_SIZE_(MiB):    512
ITERATIONS:          25

MPI_PER_NODE:        2
MPI_PARALLELISM:     4

WORKER_NODES:        2
CPUS_PER_WORKER:     30
WORKER_PARALLELISM:  60


SLURM_JOB_ID:        10772538
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 10772538 ON ilk-109 CANCELLED AT 2025-02-26T00:59:50 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 10772538.6 ON ilk-114 CANCELLED AT 2025-02-26T00:59:50 DUE TO TIME LIMIT ***
