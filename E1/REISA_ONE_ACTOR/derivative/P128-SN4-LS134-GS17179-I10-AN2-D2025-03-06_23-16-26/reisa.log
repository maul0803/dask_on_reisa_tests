slurmstepd: info: Setting TMPDIR to /scratch/10895252. Previous errors about TMPDIR can be discarded
Slurm job started at 07/03/2025_02:00:01 AM

In transit.
Initing Ray (1 head node + 2 worker nodes + 4 simulation nodes) on nodes: 
	ilk-160 ilk-164 ilk-166 ilk-169 ilk-170 ilk-171 ilk-173
Head node: 172.30.204.16:6379


RAY_DEPLOY_TIME:     28.02620
2025-03-07 02:00:32,093	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26'.
2025-03-07 02:00:32,119	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_5a266c4471dbc326.zip' (0.48MiB) to Ray cluster...
2025-03-07 02:00:32,125	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_5a266c4471dbc326.zip'.
RAY_INIT_TIME:       3.41216
Iter [0]
Iter [1]
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
GLOBAL_PUT_TIME:     8.029930353164673 (avg:0.80299)
ACTOR_CONCURRENCY:   8
SIMULATION_TIME:     49.379447589000002 (avg: 4.937944758900001)
SIM_WTHOUT_PDI:      27.385420178999997 (avg: 2.738542017900000)
PDI_DELAY:           21.994027410000005 (avg: 2.199402741000001)

GLOBAL_SIZE_(GiB):   16777216
LOCAL_SIZE_(MiB):    128
ITERATIONS:          10

MPI_PER_NODE:        32
MPI_PARALLELISM:     128

WORKER_NODES:        2
CPUS_PER_WORKER:     64
WORKER_PARALLELISM:  128


SLURM_JOB_ID:        10895252
EST_ANALYTICS_TIME:  694.86635 (avg:69.48664)
Traceback (most recent call last):
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/derivative.py", line 64, in <module>
    result = handler.get_result(process_func, iter_func, global_func=global_func, selected_iters=iterations, kept_iters=5, timeline=False)
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/reisa.py", line 154, in get_result
    return global_func(RayList(results))
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/derivative.py", line 57, in global_func
    return np.average(final_results[:])
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/reisa.py", line 51, in __getitem__
    return ray.get(RayList(item))
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return getattr(ray, func.__name__)(*args, **kwargs)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/api.py", line 42, in get
    return self.worker.get(vals, timeout=timeout)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/worker.py", line 434, in get
    res = self._get(to_get, op_timeout)
  File "/home/ulc/cursos/curso341/.local/lib/python3.9/site-packages/ray/util/client/worker.py", line 462, in _get
    raise err
ray.exceptions.RayTaskError: [36mray::iter_task()[39m (pid=1927230, ip=172.30.204.20)
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/reisa.py", line 144, in iter_task
    return iter_func(i, RayList(current_result))
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/derivative.py", line 47, in iter_func
    return np.average(current_results[:])
  File "/scratch/10895252/ray/session_2025-03-07_02-00-09_137462_2800514/runtime_resources/working_dir_files/_ray_pkg_5a266c4471dbc326/reisa.py", line 51, in __getitem__
    return ray.get(RayList(item))
ray.exceptions.RayTaskError: [36mray::process_task()[39m (pid=1927528, ip=172.30.204.20)
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/reisa.py", line 125, in process_task
    return process_func(rank, i, queue)
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/derivative.py", line 34, in process_func
    gt = np.array(queue[-5:])
  File "/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E1/REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-06_23-16-26/reisa.py", line 51, in __getitem__
    return ray.get(RayList(item))
ray.exceptions.ObjectFetchTimedOutError: Failed to retrieve object 00ffffffffffffffffffffffffffffffffffffff6300000003000000. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.

Fetch for object 00ffffffffffffffffffffffffffffffffffffff6300000003000000 timed out because no locations were found for the object. This may indicate a system-level bug.
srun: error: ilk-164: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=10895252.8

Slurm job finished at 07/03/2025_02:12:11 AM

*****************************************************************************
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                                                                           *
*****************************************************************************

Job ID: 10895252
Cluster: finisterrae3
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
Nodes: 7
Cores per node: 64
CPU Utilized: 01:23:01
CPU Efficiency: 1.52% of 3-18:58:08 core-walltime
Job Wall-clock time: 00:12:11
Memory Utilized: 669.83 MB
Memory Efficiency: 0.04% of 1.48 TB

 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
*****************************************************************************







*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                    JOB EFFICIENCY REPORT (seff 10895252)                   *
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*                                                                           *
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************






Job ID: 10895252
Job ID: 10895252
Job ID: 10895252
Job ID: 10895252
Job ID: 10895252
Job ID: 10895252
Cluster: finisterrae3
Cluster: finisterrae3
Cluster: finisterrae3
Cluster: finisterrae3
Cluster: finisterrae3
Cluster: finisterrae3
User/Group: curso341/ulc
User/Group: curso341/ulc
User/Group: curso341/ulc
User/Group: curso341/ulc
User/Group: curso341/ulc
User/Group: curso341/ulc
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
State: COMPLETED (exit code 0)
Nodes: 7
Nodes: 7
Nodes: 7
Nodes: 7
Nodes: 7
Nodes: 7
Cores per node: 64
Cores per node: 64
Cores per node: 64
Cores per node: 64
Cores per node: 64
Cores per node: 64
CPU Utilized: 01:56:51
CPU Utilized: 01:56:51
CPU Utilized: 01:56:51
CPU Utilized: 01:56:51
CPU Utilized: 01:56:51
CPU Utilized: 01:56:51
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
CPU Efficiency: 2.14% of 3-18:58:08 core-walltime
Job Wall-clock time: 00:12:11
Job Wall-clock time: 00:12:11
Job Wall-clock time: 00:12:11
Job Wall-clock time: 00:12:11
Job Wall-clock time: 00:12:11
Job Wall-clock time: 00:12:11
Memory Utilized: 15.08 GB
Memory Utilized: 15.08 GB
Memory Utilized: 15.08 GB
Memory Utilized: 15.08 GB
Memory Utilized: 15.08 GB
Memory Utilized: 15.08 GB
Memory Efficiency: 0.99% of 1.48 TB
Memory Efficiency: 0.99% of 1.48 TB
Memory Efficiency: 0.99% of 1.48 TB
Memory Efficiency: 0.99% of 1.48 TB
Memory Efficiency: 0.99% of 1.48 TB
Memory Efficiency: 0.99% of 1.48 TB






 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++   Memory Efficiency is too small. Please review the requested memory. ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ It seems that you do not need that much memory so we recommend        ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 ++ requesting less memory in other similar jobs.                         ++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 
 
 
 
 
 
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************
*****************************************************************************






