slurmstepd: info: Setting TMPDIR to /scratch/10927313. Previous errors about TMPDIR can be discarded
Slurm job started at 10/03/2025_11:20:30 AM

In transit.
Initing Ray (1 head node + 2 worker nodes + 4 simulation nodes) on nodes: 
	ilk-172 ilk-173 ilk-175 ilk-177 ilk-180 ilk-181 ilk-183
Head node: 172.30.204.28:6379


RAY_DEPLOY_TIME:     23.73527
2025-03-10 11:20:59,304	INFO packaging.py:520 -- Creating a file package for local directory '/mnt/lustre/scratch/nlsas/home/ulc/cursos/curso341/dask_on_reisa_tests/E4/DASK_ON_REISA_ONE_ACTOR/derivative/P128-SN4-LS134-GS17179-I10-AN2-D2025-03-10_08-47-45'.
2025-03-10 11:20:59,329	INFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_7c0a14cb10a7b028.zip' (0.48MiB) to Ray cluster...
2025-03-10 11:20:59,337	INFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_7c0a14cb10a7b028.zip'.
RAY_INIT_TIME:       5.26484
Iter [0]
Iter [1]
Iter [2]
Iter [3]
Iter [4]
Iter [5]
Iter [6]
Iter [7]
Iter [8]
Iter [9]
GLOBAL_PUT_TIME:     7.466095209121704 (avg:0.74661)
ACTOR_CONCURRENCY:   8
SIMULATION_TIME:     229.778121981999988 (avg: 22.977812198199999)
SIM_WTHOUT_PDI:      43.642976786000034 (avg: 4.364297678600003)
PDI_DELAY:           186.135145195999939 (avg: 18.613514519599995)

GLOBAL_SIZE_(GiB):   16777216
LOCAL_SIZE_(MiB):    128
ITERATIONS:          10

MPI_PER_NODE:        32
MPI_PARALLELISM:     128

WORKER_NODES:        2
CPUS_PER_WORKER:     64
WORKER_PARALLELISM:  128


SLURM_JOB_ID:        10927313
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 10927313 ON ilk-172 CANCELLED AT 2025-03-10T11:40:52 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 10927313.8 ON ilk-173 CANCELLED AT 2025-03-10T11:40:52 DUE TO TIME LIMIT ***
